# The Rising Costs of AI
## Values (in all sense of the word) should be baked in, not bolted on. Waterstons Innovation: The Dots #15

Whether you look at its financial, ethical, or ecological costs, AI is a very expensive tool. It’s also a tool where a lot of the use cases we hear for it do not drive actual business value, or where using it does not actually lead to a system being more useful to us. If we want to propose a productive application for AI, we need a strong handle on what it will cost us, otherwise, how do we know if it’s worth it? The price of AI has been obfuscated, but we can be certain it’s high. This week we look at what contributes to this price, what we need to think about when thinking about the cost of AI, and can AI ever cost a price we can afford?

What is this? This is The Dots, our newsletter about exciting things we find in the world of innovation. We imagine innovation as connecting the dots; putting together a jigsaw. Our puzzle pieces are the pieces of interesting information we absorb in the world, our partners and their products. Innovation happens when we connect these pieces in new and interesting ways. This newsletter is about these dots we find and connect.

### Costs
It’s easy to feel that AI is cheap - when you play with ChatGPT, or generate images with Midjourney, it is often free for you to do. Typically, you get a nice long trial or, in the case of Midjourney, an experience not as slick as the paid version. Having AI be easy to access has contributed to its current popularity, but has also led to a misconception that it is also cheap.

AI is expensive, but those costs are often obfuscated. Here, I mean financially, but it extends also into the ethical and ecological costs it carries.

Finding the true value of adopting AI is therefore difficult - we are constantly told how the value of everything will be improved by AI, but how can that be the case if you don’t know what it costs?

In cyber security, there is a concept that security should be “baked-in” and not “bolted-on”. It means that right at the start of your project, you need to consider its security, not 12 months down the road. The same is true with AI - its value (whether its financial value or how your ethical values align with it) needs to be baked-in, not bolted-on.

Let's take a look at some evidence:

### Money
As mentioned above, a common misconception is that AI is financially cheap. It really is not. This comes from a simple fact: the hardware necessary to run an AI model is expensive to run, and you need a lot of that hardware to run (and train) a model correctly.

Some numbers: getting access to Github Copilot (an AI tool run by Microsoft to help coders) costs a user $20 a month. However, it is reported1 that Microsoft actually loses $80 a month in this scenario. This is for a tool that only helps you code, and it may have to cost $100 to break even.

Microsoft is rolling out Copilot across all its tools. This will help you in PowerPoint, Word, or Power BI. Pricing is still being finalised, but right now it will cost a business $30 a month, and you have to buy 300 licenses (so, $9,000 a month). This is on top of the license a business needs to buy to simply access Microsoft 365. For context, that license costs around $30 a month too. Accessing Copilot, at the moment, will cost the same as accessing Word, Excel, Powerpoint, Outlook, OneNote, SharePoint, OneDrive, Teams and Windows.


This will cost you $108,000 a year.
Anecdotally, we built a proof-of-concept app using GPT4. It was a simple chatbot. During internal testing, over the space of 2 days, we spent £10. That may not sound like a lot - but imagine how that scales across a year, or hundreds of users. It will take some ingenuity to bring costs down. It’s hard to see how naively applying AI to a problem can be made to be cheap.

The honest situation is that AI is a new technology. What it actually costs (and what people are willing to pay) is still being figured out. Companies are rushing to add AI into their services - but these additions tend to be made from guesses at what your challenges are, rather than solutions that are designed to specifically solve your challenges. It’s a subtle difference I am making, but it’s an important one. However, how can it be any other way? AI is so emerging, has there been enough time for them to identify the best places to apply AI?

Maybe an AI tool is worth $30 a month - but you really need to be confident it’s solving a valuable problem.

### People
It’s easy to focus only on numbers, and not the human cost AI has to. Companies tend to have long lists of their values on their website that have been debated and refined to match their culture2. How do these align with the costs of using an AI model?

Training an AI model is a process, one that extends beyond funneling data into a computer. The models we interact with (like ChatGPT) are tuned to try to reduce the likelihood of them saying something unacceptable. An element of that is that ChatGPT has to understand what toxic language is, so it can avoid saying it itself. This requires a very manual process.

Facebook has a (slightly) similar process - it has to hire a team to moderate Facebook and remove toxic posts. Facebook is now being sued for the effects that this work has had on its staff3. Staff had to comb through thousands of images of murder, rape, suicide and more, leading to around 5,000 people being eligible for extra pay related to mental health issues directly linked to their work at Facebook.

So, with ChatGPT, a similar process has been created. People have to create datasets of text and imagery to teach ChatGPT what it means to be toxic. However, this work is not being done by OpenAI directly - instead, it outsources this work to Kenya, where workers do it for $2 an hour, out of sight of Silicon Valley4.

### The Environment
AI carries with it an enormous environmental cost. How does this align with a company’s ESG targets?

Partly, this comes from having to run enormous data centres, around the clock, forever. The impact of data centres on the environment is not clear.

A more direct example is water usage. Data centres get hot and the computers within them need to be cooled. A way to do this is to use water-cooling techniques. Microsoft has an enormous water bill for this reason, water has to be spent to keep an AI model going. Another way of thinking about this is that ChatGPT uses up to half a litre of water for every 5 prompts it is given5.

Microsoft has been accused by towns like West Des Moines, Iowa of being secretive. West Des Moines is the home of many of the supercomputers used to train AI. It is also an area that is a “stressed water system” - one where summer droughts have led to canoes being unable to paddle in the rivers there. Water is not an infinite resource - the issue of droughts is compounded by the fact that water used to cool a data centre is no longer drinkable without another treatment process6

### So?
If AI is going to be a part of our processes and systems, we should be realistic about the cost of it. We need to really consider whether the use case we have for it is valuable enough in the face of the financial impact it has.

Applying AI naively, for the sake of doing it, is not an effective strategy. I’m certainly not saying that you should never use it. I’m advocating for using it responsibly, for having a deep think about the challenges you face and how AI can be used as a tool to overcome them.

There’s a desire from companies to rush into adopting AI - there is an excitement, and people know tools exist to help them do it. But often the bit that is missing is understanding where the value of using AI actually is. Being rational, smart with how we apply the tools available, and driven to solve a real problem is how we can make sure that the cost of using AI is worth it.

## You Might Have Missed It…
This week, under the cover of the OpenAI madness, a few companies dropped some bad news

Meta disbanded its Responsible AI team: Meta claims they want to make AI “accountability, transparency, safety, privacy, and more”. Their Responsible AI team, which was claimed to have “little autonomy” to begin with, has now been disbanded.

Kyle Vogt, CEO of Robotaxi Developer Cruise, Resigns… Robotaxi service Cruise has been facing tough questions over the safety record of its fleet of self-driving taxis. Sam Altman was not actually the only CEO of a large tech company that left this week - Kyle Vogt, CEO of Cruise, also stepped down.

## The Next Start-Up Unicorn
Here is Gandalf.ai, a game about trying to get an AI chatbot to tell you a secret it’s keeping. How about a game like that, but where you are trying to convince a genie that you are allowed to wish for more wishes

